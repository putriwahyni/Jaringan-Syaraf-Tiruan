# -*- coding: utf-8 -*-
"""Salinan 5 dari JST - CNN - Kelompok

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Oofxnz5HwyokKrfQNILbaBBadg6acW3W
"""

from google.colab import drive
drive.mount("/content/gdrive")

data = '/content/gdrive/MyDrive/JST/Human Emotion Classification'

import os

train_data = os.path.join(data, 'train')
test_data = os.path.join(data, 'test')

# Directory with our training pictures
train_angry_data = os.path.join(train_data, 'angry')
train_happy_data = os.path.join(train_data, 'happy')
train_sad_data = os.path.join(train_data, 'sad')

# Directory with our testing pictures
test_angry_data = os.path.join(test_data, 'angry')
test_happy_data = os.path.join(test_data, 'happy')
test_sad_data = os.path.join(test_data, 'sad')

print('Total Training Images')
print('total training Angry images  :', len(os.listdir(train_angry_data)))
print('total training Happy images  :', len(os.listdir(train_happy_data)))
print('total training Sad images    :', len(os.listdir(train_sad_data)))
print('\nTotal Testing Images')
print('total testing Angry images   :', len(os.listdir(test_angry_data)))
print('total testing Happy images   :', len(os.listdir(test_happy_data)))
print('total testing Sad images     :', len(os.listdir(test_sad_data)))

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# Parameters for our graph; we'll output images in a 4x4 configuration
nrows = 5
ncols = 5

# Index for iterating over images
pic_index = 0

train_angry_fnames = os.listdir(train_angry_data)
train_happy_fnames = os.listdir(train_happy_data)
train_sad_fnames = os.listdir(train_sad_data)

# Set up matplotlib fig, and size it to fit 2x2 pics
fig = plt.gcf()
fig.set_size_inches(ncols * 2, nrows * 2)

pic_index += 8
next_angry_pix = [os.path.join(train_angry_data, fname)
                for fname in train_angry_fnames[pic_index-8:pic_index]]
next_happy_pix = [os.path.join(train_happy_data, fname)
                for fname in train_happy_fnames[pic_index-8:pic_index]]
next_sad_pix = [os.path.join(train_sad_data, fname)
                for fname in train_sad_fnames[pic_index-8:pic_index]]

for i, img_path in enumerate(next_angry_pix + next_happy_pix + next_sad_pix):
  # Set up subplot; subplot indices start at 1
  sp = plt.subplot(nrows, ncols, i + 1)
  sp.axis('Off') # Don't show axes (or gridlines)

  img = mpimg.imread(img_path)
  plt.imshow(img)

plt.show()

"""# Eksperimen"""

batch_size = 20
learning_rate = 0.01
epoch = 15

"""# Image Data Generator"""

from keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf
import numpy


img_size = (128, 128)

train_datagen = ImageDataGenerator(
                validation_split=0.2,
                rescale=1./255)

test_datagen = ImageDataGenerator(rescale=1./255)


#Create Image Generator for training and validation

train_generator=train_datagen.flow_from_directory(
    train_data,
    target_size = img_size,
    batch_size = batch_size,
    class_mode = 'categorical',
    subset = 'training')

validation_generator=train_datagen.flow_from_directory(
    train_data,
    target_size = img_size,
    batch_size = batch_size,
    class_mode = 'categorical',
    subset = 'validation')

#Create Image Generator for testing
test_data_generator = test_datagen.flow_from_directory(
        test_data,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical')

"""# CREATE MODEL"""

import tensorflow as tf
from tensorflow import keras
from keras.layers import Dense, Activation,Dropout,Conv2D, MaxPool2D,BatchNormalization, Flatten,Input
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from keras.applications import ResNet50, VGG19, InceptionV3, NASNetMobile, Xception, MobileNetV2, DenseNet201, EfficientNetV2L

i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.Xception(input_shape=(128,128,3), include_top=False, weights = 'imagenet')    # Diubah model
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

model = tf.keras.models.Sequential([
    Xception(input_shape=(128,128,3), include_top=False, weights = 'imagenet'),    # Ubah Model
])
for layer in model.layers:
  layer.trainable = False

# Model for 4 layer
model.add(Conv2D(64, (3,3),padding="same", activation='relu'))
model.add(MaxPooling2D(2,2))
model.add(Flatten())
model.add(Dense(3, activation='softmax'))

# Model for 6 layer
# model.add(Conv2D(64, (3,3), padding="same", activation='relu'))
# model.add(Conv2D(64, (3,3), padding="same", activation='relu'))
# model.add(MaxPooling2D(2,2))
# model.add(Flatten())
# model.add(Dense(64, activation='relu'))
# model.add(Dense(3, activation='softmax'))

# Model for 8 layer
# model.add(Conv2D(64, (3,3),padding="same", activation="relu"))
# model.add(MaxPool2D((2,2)))
# model.add(Conv2D(128, (3,3), padding="same", activation="relu"))
# model.add(Conv2D(128, (3,3), padding="same", activation="relu"))
# model.add(MaxPool2D((2,2)))
# model.add(Flatten())
# model.add(Dense(64, activation='relu'))
# model.add(Dense(3, activation='softmax'))


model.summary()
model.compile(loss='categorical_crossentropy',
              optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
              metrics=['accuracy'])

history = model.fit(train_generator, epochs=epoch, validation_data=validation_generator)

import matplotlib.pyplot as plt

# Retrieve a list of accuracy results on training and validation data
# sets for each training epoch
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

# Retrieve a list of list results on training and validation data
# sets for each training epoch
loss = history.history['loss']
val_loss = history.history['val_loss']

# Get number of epochs
epochs = range(len(acc))

# Plot training and validation accuracy per epoch
plt.plot(epochs,acc,'r',label='training')
plt.plot(epochs,val_acc,'b',label='validation')
plt.legend()
plt.title('Akurasi')

plt.figure()

# Plot training and validation loss per epoch
#plt.plot(epochs, loss)
#plt.plot(epochs, val_loss)

plt.plot(epochs,loss,'r',label='training')
plt.plot(epochs,val_loss,'b',label='validation')
plt.legend()
plt.title('Validation loss')

"""# Prediksi Data Uji"""

test_steps_per_epoch = numpy.math.ceil(test_data_generator.samples / test_data_generator.batch_size)

predictions = model.predict_generator(test_data_generator, steps=test_steps_per_epoch)
# Get most likely class
predicted_classes = numpy.round(predictions, 0)

from keras import metrics
true_classes = test_data_generator.classes
class_labels = ['Angry', 'Happy', 'Sad']

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

predicted_classes = np.argmax(predictions, axis=1)  # Assuming probabilities
report = classification_report(true_classes, predicted_classes, target_names=class_labels, digits=4)
print(report)